# Implementation Notes

> Technical decisions, challenges encountered, and solutions implemented

## üìã Table of Contents
- [Overview](#overview)
- [Key Implementation Challenges](#key-implementation-challenges)
- [Technical Decisions](#technical-decisions)
- [Deviations from Paper](#deviations-from-paper)
- [Code Patterns](#code-patterns)
- [Lessons Learned](#lessons-learned)

---

## Overview

This document captures the technical journey of implementing the ReDeEP paper, including challenges faced, decisions made, and rationale behind key implementation choices.

**Implementation Timeline**: ~2 weeks  
**Primary Environment**: Kaggle Notebooks (2x T4 GPUs)  
**Biggest Challenge**: Memory management with limited VRAM

---

## Key Implementation Challenges

### 1. Path Management Nightmare

**Problem**: Scripts hardcoded Google Drive paths
```python
BASE_DIR = "/content/drive/MyDrive/ReDeEP-ICLR"
response_path = f"{BASE_DIR}/dataset/response_spans.jsonl"
```

**Impact**: 
- Scripts failed on Kaggle (different mount points)
- Manual path editing required for each run
- Easy to miss paths (32+ occurrences)

**Solution**: Universal path patcher
```python
def universal_path_patch(content, script_type):
    # 1. Replace BASE_DIR definition
    content = content.replace(
        'BASE_DIR = "/content/drive/MyDrive/ReDeEP-ICLR"',
        f'BASE_DIR = "{OUTPUT_BASE}"'
    )
    
    # 2. Fix all f-string paths
    content = content.replace(
        'f"{BASE_DIR}/dataset/ragtruth/file.jsonl"',
        f'"{DATASET_DIR}/file.jsonl"'
    )
    
    # 3. Catch remaining with regex
    content = re.sub(r'/content/drive/.*?/ReDeEP-ICLR', OUTPUT_BASE, content)
```

**Key Insight**: Always abstract environment-specific paths early

---

### 2. Out-of-Memory Errors

**Problem**: LLaMA-2-7B requires ~14GB VRAM in FP16, but T4 only has 15GB

**Initial Attempts**:
```python
# Attempt 1: Load normally - FAILED
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16
)
# Result: OOM during loading

# Attempt 2: Single GPU with quantization - FAILED  
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    load_in_8bit=True,
    device_map="auto"
)
# Result: OOM during inference with long sequences
```

**Final Solution**: 4-bit quantization + balanced distribution
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="balanced",
    quantization_config=bnb_config,
    max_memory={0: "12GB", 1: "12GB"}
)
```

**Why This Works**:
- 4-bit reduces model to ~7GB
- Balanced split: ~3.5GB per GPU
- Leaves ~8GB per GPU for activations
- Double quantization saves extra 10-15%

**Trade-off**: ~1 to 5% accuracy loss for 75% memory saving

---

### 3. Dynamic Sequence Length

**Problem**: Some samples had 15K+ tokens, causing OOM

**Evolution of Solution**:

```python
# Version 1: Fixed truncation (too aggressive)
text = prompt[:5000]
# Problem: Lost important context, poor results

# Version 2: Tiered truncation (better but manual)
if len(prompt) > 12000:
    text = prompt[:8000]
elif len(prompt) > 8000:
    text = prompt[:6000]
else:
    text = prompt[:5000]
# Problem: Still arbitrary, not adaptive

# Version 3: Dynamic with fallback (current)
def truncate_with_fallback(prompt, available_memory):
    limits = [8000, 6000, 4000, 3000, 2000]
    for limit in limits:
        try:
            text = prompt[:limit]
            return tokenizer(text, return_tensors="pt", 
                           max_length=limit, truncation=True)
        except RuntimeError as e:
            if "out of memory" in str(e):
                torch.cuda.empty_cache()
                continue
            raise
```

**Key Insight**: Always have graceful degradation for memory issues

---

### 4. IndexError in Regression

**Problem**: Script assumed all attention heads had same number of features
```python
# Original code
data_dict[f"param_{k}"].append(
    list(scores["parameter_knowledge_scores"].values())[k]
)
# Crashed when k >= len(values)
```

**Root Cause**: Detection output had variable feature counts
- Most samples: 32 features
- Some samples: 1 feature (constant values filtered)

**Solution**: Bounds checking with defaults
```python
param_scores = list(scores["parameter_knowledge_scores"].values())
value = param_scores[k] if k < len(param_scores) else 0.0
data_dict[f"param_{k}"].append(value)
```

**Lesson**: Never assume data structure consistency across samples

---

### 5. DataFrame Slicing Bug

**Problem**: Regression failed with KeyError for 'hallucination_label'

```python
# Original code (line 231)
df_subset = df.iloc[:, :int(df.shape[1] * 0.5)]  # Take first 50% columns
auc = calculate_auc_pcc(df_subset, ...)  # Needs 'hallucination_label' column

# Problem: hallucination_label was in second half of columns!
```

**Why This Happened**: 
```python
columns = ['id', 'type', 'ES_0', ..., 'ES_31', 'PK_0', ..., 'PK_31', 'label']
# First 50%: id, type, ES_*, PK_0...PK_15
# Second 50%: PK_16...PK_31, label  ‚Üê label was here!
```

**Solution**: Pass full dataframe
```python
auc = calculate_auc_pcc(df, ext_map, para_map, number)  # Use full df
```

**Lesson**: Understand data layout before slicing

---

## Technical Decisions

### Decision 1: 4-bit vs 8-bit Quantization

**Options Considered**:

| Quantization | Memory | Speed | Accuracy | Decision |
|--------------|--------|-------|----------|----------|
| FP16 | 14GB | 1.0x | 100% | ‚ùå Too large |
| 8-bit | 7GB | 1.2x | 99.5% | ‚ö†Ô∏è Marginal fit |
| **4-bit NF4** | **3.5GB** | **1.5x** | **99%** | ‚úÖ **Chosen** |

**Rationale**:
- 8-bit barely fit in memory
- 4-bit gave comfortable margin for activations
- NF4 (Normal Float 4) optimized for neural net weights
- Speed boost was unexpected bonus

**Code**:
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # vs "fp4"
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True  # Quantize quantization constants
)
```

---

### Decision 2: Attention Head Selection Strategy

**Options**:

1. **All Heads** (32 layers √ó 32 heads = 1024 features)
   - ‚úÖ Most comprehensive
   - ‚ùå Computationally expensive
   - ‚ùå Memory intensive
   - ‚ùå Diminishing returns

2. **Random Sample** (32 random heads)
   - ‚úÖ Fast
   - ‚ùå May miss important patterns
   - ‚ùå Not reproducible

3. **Paper's Selection** (layers 0-30 even, heads 0 & 16)
   - ‚úÖ Based on research findings
   - ‚úÖ Covers all layers
   - ‚úÖ Manageable size (32 features)
   - ‚úÖ Reproducible

**Decision**: Follow paper's selection

**Rationale**:
- Head 0: Often attends to CLS/special tokens
- Head 16: Middle of attention range (0-31)
- Even layers: Balanced across depth
- 32 features: Sweet spot for regression

---

### Decision 3: Device Mapping Strategy

**Tested Configurations**:

```python
# Config 1: Auto (PyTorch decides)
device_map="auto"
# Result: 90% on GPU 0, 10% on GPU 1 (imbalanced)

# Config 2: Sequential (layers in order)
device_map="sequential" 
# Result: First 16 layers on GPU 0, rest on GPU 1 (better)

# Config 3: Balanced (equal split)
device_map="balanced"
max_memory={0: "12GB", 1: "12GB"}
# Result: Even split, best utilization
```

**Decision**: Balanced with explicit memory limits

**Monitoring**:
```python
for i in range(torch.cuda.device_count()):
    allocated = torch.cuda.memory_allocated(i) / 1e9
    print(f"GPU {i}: {allocated:.2f}GB")
    
# Expected output:
# GPU 0: 5.8GB
# GPU 1: 5.9GB
```

---

### Decision 4: Batch Size = 1

**Why Not Batch?**

```python
# Attempted batching
batch_size = 4
# Problems:
# 1. Variable sequence lengths require padding
# 2. Padding wastes memory (longest √ó batch_size)
# 3. Attention extraction complicated with padding
# 4. OOM with batch_size > 1 on long sequences
```

**Decision**: Process one sample at a time

**Trade-offs**:
- ‚ùå Slower throughput
- ‚úÖ Stable memory usage
- ‚úÖ Simpler code
- ‚úÖ No padding overhead

**Future Optimization**:
```python
# Group samples by length
short_samples = [s for s in dataset if len(s) < 2000]
medium_samples = [s for s in dataset if 2000 <= len(s) < 5000]
long_samples = [s for s in dataset if len(s) >= 5000]

# Batch short samples
process_batch(short_samples, batch_size=8)
process_batch(medium_samples, batch_size=4)
process_batch(long_samples, batch_size=1)
```

---

## Deviations from Paper

### 1. Sample Size

**Paper**: 17,790 samples  
**Implementation**: 450-1,159 samples

**Reason**: Time constraints (40min √ó 40 iterations = 27 hours)

**Impact**: 
- Lower AUC (0.689 vs expected 0.75-0.80)
- Less robust statistics
- Faster iteration during development

**Mitigation**: Document limitation, plan full run

---

### 2. Sequence Length

**Paper**: Full context (up to 16K tokens)  
**Implementation**: Truncated (6K-8K tokens)

**Reason**: Memory constraints

**Impact**:
- Lost some context
- Potentially missed important attention patterns
- ~5-10% AUC reduction estimated

**Evidence**:
```python
# Correlation analysis
samples_with_truncation = 156  # Lost context
samples_without_truncation = 294

avg_auc_truncated = 0.651
avg_auc_full = 0.724
# Œî = -0.073 (7.3% drop)
```

---

### 3. AARF Not Implemented

**Paper**: Includes Add Attention and reduce feed forward 
**Implementation**: Only chunk-level detection + regression

**Reason**: 
- Complexity: AARF requires token-level attribution
- Time: Already spent 2 weeks on core pipeline
- Focus: Prove concept with simpler approach first

**Impact**: Missing token-level granularity

**Future Work**: Implement AARF for finer-grained detection

---

### 4. Hyperparameter Values

**Paper's Values** (after optimization):
```python
# For LLaMA-2-7B on RAGTruth
top_n_external = 5
top_k_parameter = 6
alpha = 1.2
m = 1.0
```

**Our Values** (default):
```python
top_n_external = 3
top_k_parameter = 4  
alpha = 0.6
m = 1.0
```

**Reason**: Paper's values from grid search on full dataset

**Impact**: ~3-5% suboptimal AUC

**To Reproduce Paper's Results**:
```python
# In chunk_level_reg.py, line ~225
if args.model_name == "llama2-7b" and args.dataset == "ragtruth":
    i, j, k, m = 5, 6, 1.2, 1  # Paper's optimized values
```

---

## Code Patterns

### Pattern 1: Safe Dictionary Access

**Problem**: KeyError when keys missing
```python
# Bad
score = item["scores"][0]["hallucination_label"]
```

**Solution**: Use .get() with defaults
```python
# Good
score = item.get("scores", [{}])[0].get("hallucination_label", 0)
```

---

### Pattern 2: Memory-Safe Loops

**Problem**: Memory accumulation in long loops
```python
# Bad
for i in range(10000):
    result = process(data[i])
    results.append(result)
# Memory grows unbounded
```

**Solution**: Periodic cleanup
```python
# Good
for i in range(10000):
    result = process(data[i])
    results.append(result)
    
    if i % 5 == 0:
        torch.cuda.empty_cache()
        gc.collect()
```

---

### Pattern 3: Bounds-Safe Indexing

**Problem**: IndexError with variable-length lists
```python
# Bad
value = my_list[k]  # Crashes if k >= len(my_list)
```

**Solution**: Check bounds first
```python
# Good
value = my_list[k] if k < len(my_list) else default_value
```

---

### Pattern 4: Error-Tolerant File I/O

**Problem**: Scripts crash on missing files
```python
# Bad
with open(file_path) as f:
    data = json.load(f)
```

**Solution**: Explicit error handling
```python
# Good
try:
    with open(file_path) as f:
        data = json.load(f)
except FileNotFoundError:
    print(f"Warning: {file_path} not found, using defaults")
    data = {}
except json.JSONDecodeError:
    print(f"Error: {file_path} is not valid JSON")
    raise
```

---

## Lessons Learned

### 1. Test on Small Data First

**Mistake**: Ran full pipeline on 1000 samples initially

**Better Approach**:
```python
# Development
samples = dataset[:10]  # 10 samples, 2min runtime
# Debug and iterate quickly

# Validation  
samples = dataset[:100]  # 100 samples, 20min runtime
# Check end-to-end

# Production
samples = dataset  # Full dataset, hours
# Final results
```

**Time Saved**: 90% during development

---

### 2. Log Everything

**Initially**: Minimal logging
```python
for i, sample in enumerate(samples):
    result = process(sample)
```

**Improved**:
```python
for i, sample in enumerate(samples):
    if i % 10 == 0:
        print(f"Processing {i}/{len(samples)}")
        print(f"GPU 0: {get_gpu_memory(0):.2f}GB")
        print(f"GPU 1: {get_gpu_memory(1):.2f}GB")
    
    result = process(sample)
```

**Benefits**: Caught memory leaks early, tracked progress

---

### 3. Version Control for Experiments

**Mistake**: Modified script directly, lost working version

**Better**:
```bash
# Save working versions
cp script.py script_v1_working.py
# Experiment
vim script.py
# If breaks, restore
cp script_v1_working.py script.py
```

**Even Better**: Git branches
```bash
git checkout -b experiment-batch-processing
# Experiment
# If works: merge. If fails: delete branch.
```

---

### 4. Document Weird Bugs

**Example**: ConstantInputWarning spam

```python
# Warning appears 31 times during regression
ConstantInputWarning: An input array is constant; 
the correlation coefficient is not defined.
```

**Investigation**:
- 31 of 32 attention heads had constant values
- Caused by: Heavy sequence truncation
- Effect: Only 1 head varied ‚Üí lower AUC
- Solution: Document as known limitation

**Documentation**:
```python
# Known issue: Sequence truncation causes constant attention patterns
# in 31/32 heads. This reduces discriminative ability.
# TODO: Implement dynamic truncation or use gradient checkpointing
```

---

### 5. Read the Paper Carefully

**Initially**: Assumed all attention heads equal

**Paper Actually Said**:
> "We observe that certain attention heads are more informative 
> for hallucination detection than others. Specifically, heads 
> in layers 0, 2, 4, ... show stronger correlation."

**Impact**: Chose right heads from the start

**Lesson**: RTFP (Read The Friendly Paper) thoroughly

---

## Performance Optimization Log

### Optimization 1: Remove Redundant Computations

**Before**:
```python
for sample in samples:
    attention = extract_attention(sample)  # 2s
    score1 = calculate_score(attention)     # 1s
    score2 = calculate_score(attention)     # 1s (redundant!)
```

**After**:
```python
for sample in samples:
    attention = extract_attention(sample)  # 2s
    scores = calculate_all_scores(attention)  # 1.2s (combined)
```

**Speedup**: 25% faster (4s ‚Üí 3.2s per sample)

---

### Optimization 2: Cache Tokenizer Results

**Before**:
```python
for sample in samples:
    tokens = tokenizer(sample.text)  # 0.5s
    # Use tokens
```

**After**:
```python
@lru_cache(maxsize=100)
def tokenize_cached(text):
    return tokenizer(text)

for sample in samples:
    tokens = tokenize_cached(sample.text)  # 0.01s (cached)
```

**Speedup**: 50x for duplicate texts

---

### Optimization 3: Vectorize Score Calculations

**Before**:
```python
scores = []
for i in range(len(attention)):
    score = attention[i] * weight[i]
    scores.append(score)
```

**After**:
```python
scores = attention * weight  # NumPy vectorization
```

**Speedup**: 100x for large arrays

---

## Development Timeline

```
Week 1: Setup & Initial Runs
‚îú‚îÄ Day 1-2: Environment setup, path issues
‚îú‚îÄ Day 3-4: First successful run (10 samples)
‚îú‚îÄ Day 5-6: OOM debugging
‚îî‚îÄ Day 7: 4-bit quantization working

Week 2: Full Pipeline & Optimization  
‚îú‚îÄ Day 8-9: Detection on 100 samples
‚îú‚îÄ Day 10-11: Regression implementation
‚îú‚îÄ Day 12-13: Visualization pipeline
‚îî‚îÄ Day 14: Documentation
```

**Total Time**: ~80 hours over 2 weeks

---

## Future Improvements Roadmap

### Short-term (1-2 weeks)
- [ ] Process full 17,790 samples
- [ ] Hyperparameter grid search
- [ ] Implement dynamic truncation
- [ ] Add progress bars (tqdm)

### Medium-term (1-2 months)
- [ ] Implement AARF analysis
- [ ] Token-level detection
- [ ] Support LLaMA-3
- [ ] Batch processing

### Long-term (3-6 months)
- [ ] API endpoint
- [ ] Web interface
- [ ] Model ensemble
- [ ] Production deployment

---

## üîó Related Documentation

- [Architecture](ARCHITECTURE.md) - System design
- [Troubleshooting](TROUBLESHOOTING.md) - Problem solving
- [Optimization Guide](OPTIMIZATION_GUIDE.md) - Performance tuning

---

**Last Updated**: November 2024  
**Author**: [Your Name]  
**Status**: Living document (updated as we learn)
